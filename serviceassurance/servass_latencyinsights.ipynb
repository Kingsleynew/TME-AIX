{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telecom Service Assurance AI Model (Transformer NN) for Latency Insights\n",
    "Author: Fatih E. NAR\n",
    "\n",
    "## Introduction\n",
    "In this notebook, we showcase a machine learning model to create latency predictions for telecom networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "%pip install -r requirements.txt\n",
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders) or CUDA is available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the generated data\n",
    "data = pd.read_csv('data/servass_data.csv.xz', compression='xz', parse_dates=['timestamp'])\n",
    "\n",
    "# Inspect the data for problematic values\n",
    "print(\"Initial data inspection:\")\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "\n",
    "# Replace string representations of empty lists with NaN\n",
    "for col in ['latency', 'jitter', 'packet_loss', 'throughput', 'cpu_usage', 'memory_usage']:\n",
    "    data[col] = data[col].replace('[]', np.nan)\n",
    "\n",
    "# Ensure all relevant columns are numeric and replace non-numeric values with NaN\n",
    "for col in ['latency', 'jitter', 'packet_loss', 'throughput', 'cpu_usage', 'memory_usage']:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Impute missing values in numeric columns instead of dropping rows\n",
    "numeric_cols = ['latency', 'jitter', 'packet_loss', 'throughput', 'cpu_usage', 'memory_usage']\n",
    "data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i + seq_length].values)\n",
    "    return np.array(sequences)\n",
    "\n",
    "seq_length = 30  # Length of the sequences (e.g., 30 time steps)\n",
    "sequences = create_sequences(data[numeric_cols], seq_length)\n",
    "\n",
    "# Ensure sequences are numeric\n",
    "for i in range(sequences.shape[0]):\n",
    "    for j in range(sequences.shape[1]):\n",
    "        for k in range(sequences.shape[2]):\n",
    "            if isinstance(sequences[i, j, k], str):\n",
    "                sequences[i, j, k] = np.nan\n",
    "\n",
    "# Convert to float32\n",
    "X = sequences[:, :-1, :].astype(np.float32)  # Input sequences\n",
    "y = sequences[:, -1, :].astype(np.float32)   # Corresponding labels\n",
    "\n",
    "# Drop any remaining NaN values\n",
    "nan_mask = ~np.isnan(X).any(axis=(1, 2)) & ~np.isnan(y).any(axis=1)\n",
    "X = X[nan_mask]\n",
    "y = y[nan_mask]\n",
    "\n",
    "# Check shapes of the datasets\n",
    "print(f'X shape: {X.shape}')\n",
    "print(f'y shape: {y.shape}')\n",
    "print(f'X_train shape: {X[:int(0.8 * len(X))].shape}')\n",
    "print(f'X_val shape: {X[int(0.8 * len(X)):].shape}')\n",
    "print(f'y_train shape: {y[:int(0.8 * len(y))].shape}')\n",
    "print(f'y_val shape: {y[int(0.8 * len(y)):].shape}')\n",
    "\n",
    "# Ensure there's sufficient data for training\n",
    "if len(X) < 32:\n",
    "    raise ValueError('Not enough data to train the model. Increase the dataset size.')\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_val = X[:train_size], X[train_size:]\n",
    "y_train, y_val = y[:train_size], y[train_size:]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block & Model Implementation\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.layernorm1(x + self.dropout(attn_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.layernorm2(x + self.dropout(ffn_output))\n",
    "        return x\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_shape, embed_dim, num_heads, ff_dim, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.dense = nn.Linear(input_shape[-1], embed_dim)\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(embed_dim, 128)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc2 = nn.Linear(128, input_shape[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        x = x.permute(1, 0, 2)  # (batch_size, seq_length, embed_dim) -> (seq_length, batch_size, embed_dim)\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x)\n",
    "        x = x.permute(1, 2, 0)  # (seq_length, batch_size, embed_dim) -> (batch_size, embed_dim, seq_length)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.squeeze(-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "input_shape = (seq_length - 1, X.shape[-1])\n",
    "model = TransformerModel(input_shape, embed_dim=64, num_heads=4, ff_dim=128, num_layers=2).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Set up mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Implement early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                if self.verbose:\n",
    "                    print(\"Early stopping\")\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with early stopping\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs in val_loader:\n",
    "        inputs = inputs[0].to(device)\n",
    "        outputs = model(inputs)\n",
    "        y_pred.append(outputs.cpu().numpy())\n",
    "y_pred = np.concatenate(y_pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale the predictions and actual values back to the original scale\n",
    "y_pred_rescaled = scaler.inverse_transform(y_pred)\n",
    "y_val_rescaled = scaler.inverse_transform(y_val)\n",
    "\n",
    "# Calculate MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "mape = mean_absolute_percentage_error(y_val_rescaled, y_pred_rescaled)\n",
    "print(f'MAPE: {mape:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(range(len(y_val_rescaled)), y_val_rescaled, label='Actual Latency')\n",
    "plt.plot(range(len(y_pred_rescaled)), y_pred_rescaled, label='Predicted Latency')\n",
    "plt.title('Actual vs Predicted Latency')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Latency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
