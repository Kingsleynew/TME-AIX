{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telco Platform Engineering Security Operations <br> AI Insights Generation with Fine Tuning of Meta's BART Model\n",
    "Author: Fatih E. NAR <br>\n",
    "This project aims to deliver a security risk factor insights <br>\n",
    "Model Card: https://huggingface.co/google-t5/t5-small <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Check if any accelerator is available \n",
    "# If you are reading this and about to buy a GPU, please buy an NVIDIA GPU as all AI frameworks are optimized for NVIDIA GPUs,\n",
    "# and you will have a better experience with NVIDIA GPUs.\n",
    "fp16 = False\n",
    "if torch.cuda.is_available():\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "    max_memory_mb = 8 * 1024\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = f'max_split_size_mb:{max_memory_mb}'\n",
    "    fp16 = True\n",
    "# Check if MPS (Apple Silicon GPU) is available\n",
    "elif torch.backends.mps.is_available():\n",
    "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Load the data\n",
    "# why did we use parquet format? Cos we love to complicate stuff. Just kidding. \n",
    "# Parquet is a columnar storage format that is optimized for reading and writing data. \n",
    "# And it is a good choice for storing large datasets.\n",
    "data = pd.read_parquet('data/5G_SecOps_Data_10K.parquet')\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(data):\n",
    "    # Convert categorical variables to numeric\n",
    "    data['Service Configurations'] = data['Service Configurations'].map({'Poor': 0, 'Medium': 1, 'Good': 2})\n",
    "    data['Service Reachability'] = data['Service Reachability'].map({'Internal': 0, 'External': 1})\n",
    "    data['RBAC Configuration'] = data['RBAC Configuration'].map({'Low': 0, 'Medium': 1, 'High': 2})\n",
    "    # Drop non-numeric columns\n",
    "    data = data.drop(columns=['Cluster', 'Namespace'])\n",
    "    return data\n",
    "\n",
    "data = preprocess_data(data)\n",
    "\n",
    "# Define features and target\n",
    "# Our target (ie do prediction that we will, says Yoda) is the Risk Score column\n",
    "X = data.drop(columns=['Risk Score'])\n",
    "y = data['Risk Score']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to string format for T5, Liquid Metal lol\n",
    "X_train = X_train.astype(str)\n",
    "X_test = X_test.astype(str)\n",
    "y_train = y_train.astype(str)\n",
    "y_test = y_test.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5RegressionDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, tokenizer, max_len=32):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = str(self.inputs[idx])\n",
    "        target_text = str(self.targets[idx])\n",
    "        input_encoding = self.tokenizer(\n",
    "            input_text, max_length=self.max_len, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_text, max_length=self.max_len, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': input_encoding['input_ids'].flatten(),\n",
    "            'attention_mask': input_encoding['attention_mask'].flatten(),\n",
    "            'labels': target_encoding['input_ids'].flatten()\n",
    "        }\n",
    "\n",
    "# Tokenize the data\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n",
    "\n",
    "train_dataset = T5RegressionDataset(X_train.values.tolist(), y_train.values.tolist(), tokenizer)\n",
    "test_dataset = T5RegressionDataset(X_test.values.tolist(), y_test.values.tolist(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and training arguments\n",
    "# we started with BART then BERT both which performed poorly and we found ourselves back to gracious arms of BigG's T5\n",
    "# Well for Fatih E. NAR BigG is the place where he had good times.\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
    "\n",
    "# Define PEFT/LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=4, # it was 4\n",
    "    lora_alpha=16, # it was 16\n",
    "    lora_dropout=0.1, # it was 0.1\n",
    "    target_modules=['q', 'v', 'k', 'o']\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.to(device)  # Language modeling head to GPU \n",
    "\n",
    "# If you GPU sucks you can reduce the epock count to 1 , \n",
    "# at that level which your loss rate shoud be around 0.05 already\n",
    "# if not you can increase the epoch count to higher.\n",
    "# if your GPU memory is also limited and endding with OOM error, you can reduce the batch size to 2 or 1\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=512,\n",
    "    gradient_accumulation_steps=64,  # Accumulate gradients over multiple steps\n",
    "    per_device_eval_batch_size=512,\n",
    "    learning_rate=3e-5,\n",
    "    logging_dir='./logs',\n",
    "    save_total_limit=10,\n",
    "    eval_strategy=\"steps\",  # Evaluate during training at each `logging_steps`\n",
    "    logging_steps=100,  # Log every 500 steps\n",
    "    eval_steps=100,  # Evaluate every 2000 steps\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"loss\",  # Use loss to evaluate the best model\n",
    "    predict_with_generate=True,\n",
    "    fp16=fp16, # Mixed precision training only for CUDA\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation results:\")\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Visualize Our Model Performance\n",
    "# Function to predict and plot actual vs. predicted risk factors\n",
    "def predict_and_plot(trainer, test_dataset, y_test):\n",
    "    # Set generation parameters\n",
    "    generation_kwargs = {\n",
    "        \"max_length\": 10,  # Adjust as needed\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"num_beams\": 1\n",
    "    }\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = trainer.predict(test_dataset, **generation_kwargs)\n",
    "    predicted_risk_scores = predictions.predictions.squeeze()\n",
    "\n",
    "    # Decode the predictions\n",
    "    predicted_risk_scores = [tokenizer.decode(ids, skip_special_tokens=True) for ids in predicted_risk_scores]\n",
    "    print(predicted_risk_scores)\n",
    "\n",
    "    # Post-process the output to extract numeric values\n",
    "    def extract_numeric(prediction):\n",
    "        try:\n",
    "            # Extract the first numeric value from the string\n",
    "            return float(''.join(filter(str.isdigit, prediction)) or np.nan)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "    predicted_risk_scores = np.array([extract_numeric(pred) for pred in predicted_risk_scores])\n",
    "    print(predicted_risk_scores)\n",
    "    # Handle any non-numeric predictions\n",
    "    valid_indices = ~np.isnan(predicted_risk_scores)\n",
    "    predicted_risk_scores = predicted_risk_scores[valid_indices]\n",
    "    actual_risk_scores = np.array([float(value) for value in y_test])[valid_indices]\n",
    "    print(actual_risk_scores)\n",
    "    # Plot actual vs. predicted risk factors\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(actual_risk_scores, actual_risk_scores, alpha=0.5, color='blue', label='Actual Risk Factors')\n",
    "    plt.scatter(actual_risk_scores, predicted_risk_scores, alpha=0.5, color='red', label='Predicted Risk Factors')\n",
    "    plt.xlabel(\"Actual Risk Factors\")\n",
    "    plt.ylabel(\"Predicted Risk Factors\")\n",
    "    plt.title(\"Actual vs. Predicted Risk Factors\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Predict and plot\n",
    "predict_and_plot(trainer, test_dataset, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As Final Point Lets Save the Model\n",
    "# Fatih E. NAR You are a genius and very handsome engineer! \n",
    "# Sometimes good to praise yourself, you know. :-)\n",
    "model_save_path = 'model/t5_risk_score_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained('model/t5_risk_score_tokenizer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
