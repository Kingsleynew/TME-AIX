{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telecom Revenue Assurance AI Model (Fine-Tuned BERT with {PEFT/LoRA})\n",
    "Author: Fatih E. NAR\n",
    "\n",
    "This notebook trains a BERT model with LoRA (Low-Rank Adaptation) for detecting fraud in a telco revenue assurance context. The training is optimized for low capacity GPU resources by using techniques such as gradient accumulation and mixed precision training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Necessary Libraries\n",
    "!pip install -r requirements.txt\n",
    "!pip install transformers datasets accelerate peft bitsandbytes torch scikit-learn accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import lzma\n",
    "import shutil\n",
    "import bitsandbytes as bnb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Extract the .xz file\n",
    "xz_file_path = 'data/telecom_revass_data.csv.xz'\n",
    "csv_file_path = 'data/telecom_revass_data.csv'\n",
    "with lzma.open(xz_file_path, 'rb') as f_in:\n",
    "    with open(csv_file_path, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders) or CUDA is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=5e-5)\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Convert data to text format\n",
    "df['text'] = df.apply(lambda row: (\n",
    "    f\"Call Duration: {row['Call_Duration']}, Data Usage: {row['Data_Usage']}, \"\n",
    "    f\"SMS Count: {row['Sms_Count']}, Roaming Indicator: {row['Roaming_Indicator']}, \"\n",
    "    f\"Mobile Wallet Use: {row['MobileWallet_Use']}, Plan Type: {row['Plan_Type']}, \"\n",
    "    f\"Cost: {row['Cost']}, Cellular Location Distance: {row['Cellular_Location_Distance']}, \"\n",
    "    f\"Personal Pin Used: {row['Personal_Pin_Used']}, Avg Call Duration: {row['Avg_Call_Duration']}, \"\n",
    "    f\"Avg Data Usage: {row['Avg_Data_Usage']}, Avg Cost: {row['Avg_Cost']}\"\n",
    "), axis=1)\n",
    "\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['Fraud'])\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'Fraud']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['text', 'Fraud']])\n",
    "\n",
    "# Rename column if it exists\n",
    "if 'Fraud' in train_dataset.column_names:\n",
    "    train_dataset = train_dataset.rename_column('Fraud', 'labels')\n",
    "if 'Fraud' in test_dataset.column_names:\n",
    "    test_dataset = test_dataset.rename_column('Fraud', 'labels')\n",
    "\n",
    "# Load pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Apply LoRA to the model\n",
    "config = LoraConfig(r=4, lora_alpha=8, lora_dropout=0.2, bias=\"none\")\n",
    "model = get_peft_model(model, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Check if CUDA is available and set fp16 accordingly\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,  # Adjusted for low capacity GPU,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=8,  # Increased to handle larger effective batch sizes,  # Accumulate gradients\n",
    "    push_to_hub=False,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    fp16=use_fp16  # Enable mixed precision training if CUDA is available  # Enable mixed precision training if using CUDA\n",
    ")\n",
    "\n",
    "# Define custom compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer_args = {\n",
    "    'model': model,\n",
    "    'args': training_args,\n",
    "    #'train_dataset': train_dataset,\n",
    "    #'eval_dataset': test_dataset,\n",
    "    'compute_metrics': compute_metrics,\n",
    "    'tokenizer': tokenizer,\n",
    "    'train_dataset': train_dataset.select(range(1000)),  # Use a subset for debugging\n",
    "    'eval_dataset': test_dataset.select(range(200)),  # Use a subset for debugging\n",
    "}\n",
    "\n",
    "if device == torch.device('cuda'):\n",
    "    trainer_args['optimizers'] = (AdamW(model.parameters(), lr=2e-5), None)  # Optimizer < enable only when using with NVIDIA\n",
    "\n",
    "trainer = Trainer(**trainer_args)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print results with formatting\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model_save_path = 'models/fine-tuned-bert-perf-revass'\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "dummy_input = {\n",
    "    \"input_ids\": torch.zeros(1, train_dataset[0]['input_ids'].shape[0], dtype=torch.long).to(device),\n",
    "    \"attention_mask\": torch.zeros(1, train_dataset[0]['attention_mask'].shape[0], dtype=torch.long).to(device)\n",
    "}\n",
    "\n",
    "onnx_model_path = 'models/fine-tuned-bert-revass.onnx'\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (dummy_input[\"input_ids\"], dummy_input[\"attention_mask\"]),\n",
    "    onnx_model_path,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\"input_ids\": {0: \"batch_size\"}, \"attention_mask\": {0: \"batch_size\"}, \"logits\": {0: \"batch_size\"}},\n",
    "    opset_version=14  # Use opset version 14\n",
    ")\n",
    "\n",
    "print(f\"Model exported to {onnx_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
