{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5G Network Operations Insights with <br> Fine Tuning of T5-Base Version-2.0\n",
    "## Project Overview\n",
    "Author: Fatih E. NAR<br>\n",
    "This project aims to deliver a 5g network insight with fine tuning a network performant encoder-decoder TransformerNN<br>\n",
    "Updates: <br>\n",
    "(1) Using Pretrained Bigger Version of T5 (t5-base)<br>\n",
    "(2) This version uses HF accelerator framework to send the model to appropriate device (instead of manual .to(device). <br>\n",
    "(3) Performing a feature engineering to create better data-mesh for model to digest to.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once\n",
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "import lzma\n",
    "import shutil\n",
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration, \n",
    "    T5Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments, \n",
    "    get_linear_schedule_with_warmup, \n",
    "    get_cosine_schedule_with_warmup,\n",
    "    Seq2SeqTrainer, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    EvalPrediction,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model_save_path = \"models/5gran_faultprediction_model\"\n",
    "model_name = \"t5-base\" #\"t5-small for less powerfull gpus\"\n",
    "\n",
    "# Set TOKENIZERS_PARALLELISM to false to avoid warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "gc.collect()\n",
    "fp16v = False # Set to True to enable mixed precision training\n",
    "\n",
    "# Check if any accelerator is available \n",
    "if torch.cuda.is_available():\n",
    "    fp16v = True\n",
    "\n",
    "# Extract the .xz file\n",
    "with lzma.open('data/5G_netops_data_100K.csv.xz', 'rb') as f_in:\n",
    "    with open('data/5G_netops_data_100K.csv', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "# Load the synthetic telecom data\n",
    "data_path = \"data/5G_netops_data_100K.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values and prepare input and target texts\n",
    "# Ensure all NaN values are filled with empty strings\n",
    "data = pd.read_csv(data_path)\n",
    "data = data.fillna('')\n",
    "\n",
    "# Define initial feature columns\n",
    "initial_numerical_features = ['Cell Availability (%)', 'MTTR (hours)', 'Throughput (Mbps)', 'Latency (ms)', \n",
    "                      'Packet Loss Rate (%)', 'Call Drop Rate (%)', 'Handover Success Rate (%)', \n",
    "                      'Alarm Count', 'Critical Alarm Count', 'Parameter Changes', \n",
    "                      'Successful Configuration Changes (%)', 'Data Usage (GB)', 'User Count', \n",
    "                      'Signal Strength (dBm)', 'Jitter (ms)', 'Connection Setup Success Rate (%)', \n",
    "                      'Security Incidents', 'Authentication Failures', 'Temperature (Â°C)', 'Humidity (%)']\n",
    "categorical_features = ['Season', 'Weather', 'City', 'State']\n",
    "\n",
    "# Feature engineering\n",
    "def select_top_features(X, y, n_features=15):\n",
    "    mi_scores = mutual_info_regression(X, y)\n",
    "    mi_scores = pd.Series(mi_scores, index=X.columns)\n",
    "    return mi_scores.nlargest(n_features).index.tolist()\n",
    "\n",
    "# Select top features\n",
    "X = data[initial_numerical_features]\n",
    "y = data['Fault Occurrence Rate (%)']\n",
    "top_features = select_top_features(X, y, n_features=8)\n",
    "print(\"Top features:\", top_features)\n",
    "\n",
    "# Create interaction features\n",
    "def create_interaction_features(df, features):\n",
    "    for i in range(len(features)):\n",
    "        for j in range(i+1, len(features)):\n",
    "            feature_name = f\"interaction_{features[i]}_{features[j]}\"\n",
    "            df[feature_name] = df[features[i]] * df[features[j]]\n",
    "    return df\n",
    "\n",
    "# Apply interaction features\n",
    "data = create_interaction_features(data, top_features)\n",
    "\n",
    "# Update numerical_features list with new features\n",
    "numerical_features = initial_numerical_features + [col for col in data.columns if col.startswith('interaction_')]\n",
    "\n",
    "print(f\"Total number of features after engineering: {len(numerical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Fit the preprocessor on the data\n",
    "preprocessor.fit(data[numerical_features + categorical_features])\n",
    "\n",
    "# Transform all data at once\n",
    "transformed_features = preprocessor.transform(data[numerical_features + categorical_features])\n",
    "\n",
    "# Get feature names\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Prepare input text efficiently\n",
    "input_texts = []\n",
    "for row in tqdm(transformed_features, desc=\"Preparing input texts\"):\n",
    "    input_texts.append(\" \".join([f\"{name}: {value}\" for name, value in zip(feature_names, row)]))\n",
    "\n",
    "# Add input_text and target_text to the dataframe\n",
    "data['input_text'] = input_texts\n",
    "\n",
    "# Normalize target variable\n",
    "data['target_text'] = (data['Fault Occurrence Rate (%)'] / 100).astype(str)\n",
    "\n",
    "# Create Dataset efficiently\n",
    "dataset = Dataset.from_pandas(data[['input_text', 'target_text']])\n",
    "\n",
    "# Split the dataset into training and evaluation\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "# Check the loaded dataset\n",
    "print(f\"Training Dataset size: {len(train_dataset)}\")\n",
    "print(f\"Evaluation Dataset size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Special tokens have been added in the vocabulary\")\n",
    "\n",
    "# Load the tokenizer from the pretrained model\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, model_max_length=512, legacy=False)\n",
    "\n",
    "# Match Tokenizer to the model (if needed)\n",
    "new_tokens = [f'<SPL_{i}>' for i in range(0,28)]\n",
    "num_added_tokens = tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# Ensure pad token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer length: {len(tokenizer)}\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# Load the model\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "if num_added_tokens > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tokenize datasets\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples['input_text'],\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Use the new recommended approach for target tokenization\n",
    "    labels = tokenizer(\n",
    "        text_target=examples['target_text'],\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing efficiently\n",
    "def tokenize_dataset(dataset):\n",
    "    return dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=4,  # Adjust based on your CPU cores\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Tokenizing dataset\"\n",
    "    ).with_format(\"torch\")\n",
    "\n",
    "train_dataset = tokenize_dataset(train_dataset)\n",
    "eval_dataset = tokenize_dataset(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine mixed precision based on CUDA availability\n",
    "mixed_precision = \"fp16\" if torch.cuda.is_available() else \"no\"\n",
    "\n",
    "# Initialize the accelerator\n",
    "accelerator = Accelerator(mixed_precision=mixed_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PEFT/LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=4, # it was 4\n",
    "    lora_alpha=16, # it was 16\n",
    "    lora_dropout=0.1, # it was 0.05\n",
    "    target_modules=['q', 'v', 'k', 'o']\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "# Prepare the model with Accelerator\n",
    "model = accelerator.prepare(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_floats(text):\n",
    "    # Extract all floating-point numbers from the text\n",
    "    return [float(num) for num in re.findall(r'-?\\d+\\.\\d+', text)]\n",
    "\n",
    "# Define custom evaluation metric\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions\n",
    "    labels = p.label_ids\n",
    "\n",
    "    # Decode the predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Clean up the decoded predictions and labels and extract floats\n",
    "    decoded_preds_clean = [extract_floats(pred) for pred in decoded_preds if pred.strip() and pred.strip() != '</s>']\n",
    "    decoded_labels_clean = [extract_floats(label) for label in decoded_labels if label.strip() and label.strip() != '</s>']\n",
    "\n",
    "    # Flatten the lists of lists and ensure they contain only floats\n",
    "    decoded_preds_float = [item for sublist in decoded_preds_clean for item in sublist]\n",
    "    decoded_labels_float = [item for sublist in decoded_labels_clean for item in sublist]\n",
    "\n",
    "    # Ensure the lists are of the same length\n",
    "    min_length = min(len(decoded_preds_float), len(decoded_labels_float))\n",
    "    decoded_preds_float = decoded_preds_float[:min_length]\n",
    "    decoded_labels_float = decoded_labels_float[:min_length]\n",
    "\n",
    "    # Compute RMSE\n",
    "    rmse = np.sqrt(np.mean((np.array(decoded_preds_float) - np.array(decoded_labels_float)) ** 2))\n",
    "    return {\"rmse\": rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom scheduler function\n",
    "def get_custom_scheduler(optimizer, num_training_steps, num_warmup_steps):\n",
    "    return get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "# Create a custom Trainer class\n",
    "class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def create_scheduler(self, num_training_steps: int, optimizer: torch.optim.Optimizer = None):\n",
    "        if self.lr_scheduler is None:\n",
    "            self.lr_scheduler = get_custom_scheduler(\n",
    "                optimizer=self.optimizer if optimizer is None else optimizer,\n",
    "                num_training_steps=num_training_steps,\n",
    "                num_warmup_steps=self.args.get_warmup_steps(num_training_steps)\n",
    "            )\n",
    "        return self.lr_scheduler\n",
    "\n",
    "# Set training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",  # Output directory\n",
    "    overwrite_output_dir=True,  # Overwrite the content of the output directory\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    per_device_train_batch_size=26,  # Batch size per device during training [26 for t5-base with 24GB GPU]\n",
    "    gradient_accumulation_steps=14,  # Accumulate gradients over multiple steps [64 for t5-base with 16K Cuda Cores]\n",
    "    per_device_eval_batch_size=32, # Batch size per device during evaluation [36 for t5-base with 24GB GPU]\n",
    "    learning_rate=2e-5,  # or 2e-5\n",
    "    save_steps=100,  # Save checkpoint every 2000 steps\n",
    "    save_total_limit=10,  # Limit the total amount of checkpoints\n",
    "    eval_strategy=\"steps\",  # Evaluate during training at each `logging_steps`\n",
    "    logging_steps=100,  # Log every 500 steps\n",
    "    eval_steps=100,  # Evaluate every 2000 steps\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"rmse\",  # Use loss to evaluate the best model\n",
    "    predict_with_generate=True,  # Use generation for evaluation\n",
    "    fp16=fp16v,  # Load mixed precision training for CUDA only\n",
    "    remove_unused_columns=False,  # Remove unused columns from the dataset\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.02,\n",
    ")\n",
    "\n",
    "# Implement early stopping\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=5)\n",
    "\n",
    "# Create Trainer instance with early stopping\n",
    "trainer = CustomSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "print(f\"Tokenizer Final Size = {len(tokenizer)}\")\n",
    "print(f\"Model Final Size = {model.get_input_embeddings().weight.shape[0]}\")\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(\"Training complete and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate(eval_dataset)\n",
    "print(\"Evaluation Results:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
