{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5G Network Operations Insights with <br> Fine Tuning of T5-Small (This is the smallest version of T5.)\n",
    "## Project Overview\n",
    "Author: Fatih E. NAR<br>\n",
    "This project aims to deliver a 5g network insight with fine tuning a network performant encoder-decoder TransformerNN<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import threading\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "from datasets import Dataset\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments, get_linear_schedule_with_warmup, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model_save_path = \"models/5gran_faultprediction_model\"\n",
    "model_name = \"t5-small\"\n",
    "\n",
    "# Set TOKENIZERS_PARALLELISM to false to avoid warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "gc.collect()\n",
    "fp16v = False # Set to True to enable mixed precision training\n",
    "parallel = False # Set to True to enable parallel training\n",
    "device = None\n",
    "\n",
    "# Check if any accelerator is available \n",
    "if torch.cuda.is_available():\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "    device = torch.device(\"cuda\")\n",
    "    fp16v = True\n",
    "    torch.cuda.empty_cache()\n",
    "    max_memory_mb = 10 * 1024 # Set the maximum memory to 10GB, Adjust this value based on the GPU memory\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = f'max_split_size_mb:{max_memory_mb}'\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        parallel = True\n",
    "        print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "# Check if MPS (Apple Silicon GPU) is available\n",
    "elif torch.backends.mps.is_available():\n",
    "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "# Extract the .xz file\n",
    "with lzma.open('data/5G_netops_data_100K.csv.xz', 'rb') as f_in:\n",
    "    with open('data/5G_netops_data_100K.csv', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "# Load the synthetic telecom data\n",
    "data_path = \"data/5G_netops_data_100K.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information about the full dataset\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values and prepare input and target texts\n",
    "# Ensure all NaN values are filled with empty strings\n",
    "data = data.fillna('')\n",
    "\n",
    "# Ensure 'Zip' column is treated as a string\n",
    "data['Zip'] = data['Zip'].astype(str)\n",
    "\n",
    "# Prepare the input_text and target_text columns\n",
    "data['input_text'] = data.apply(lambda row: f\"Season: {row['Season']} Cell Availability: {row['Cell Availability (%)']} MTTR: {row['MTTR (hours)']} Throughput: {row['Throughput (Mbps)']} Latency: {row['Latency (ms)']} Packet Loss Rate: {row['Packet Loss Rate (%)']} Call Drop Rate: {row['Call Drop Rate (%)']} Handover Success Rate: {row['Handover Success Rate (%)']} Alarm Count: {row['Alarm Count']} Critical Alarm Count: {row['Critical Alarm Count']} Parameter Changes: {row['Parameter Changes']} Successful Configuration Changes: {row['Successful Configuration Changes (%)']} Data Usage: {row['Data Usage (GB)']} User Count: {row['User Count']} Signal Strength: {row['Signal Strength (dBm)']} Jitter: {row['Jitter (ms)']} Connection Setup Success Rate: {row['Connection Setup Success Rate (%)']} Security Incidents: {row['Security Incidents']} Authentication Failures: {row['Authentication Failures']} Temperature: {row['Temperature (Â°C)']} Humidity: {row['Humidity (%)']} Weather: {row['Weather']} Issue Reported: {row['Issue Reported']} City: {row['City']} State: {row['State']} Zip: {row['Zip']}\", axis=1)\n",
    "data['target_text'] = data['Fault Occurrence Rate (%)'].astype(str)\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Split the dataset into training and evaluation\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "# Check the loaded dataset\n",
    "print(f\"Training Dataset size: {len(train_dataset)}\")\n",
    "print(f\"Evaluation Dataset size: {len(eval_dataset)}\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from the pretrained model\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "# Match Tokenizer to the model\n",
    "tokenizer.add_tokens([f'<SPL_{i}' for i in range(0,28)])\n",
    "# Add the pad token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "else:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "#print len of tokenizer\n",
    "print(len(tokenizer))\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize datasets\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['input_text']\n",
    "    targets = examples['target_text']\n",
    "    model_inputs = tokenizer(inputs, max_length=512, padding='max_length', truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, padding='max_length', truncation=True)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "columns = ['input_ids', 'attention_mask', 'labels']\n",
    "train_dataset.set_format(type='torch', columns=columns)\n",
    "eval_dataset.set_format(type='torch', columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PEFT/LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=4, # it was 2\n",
    "    lora_alpha=32, # it was 16\n",
    "    lora_dropout=0.1, # it was 0.05\n",
    "    target_modules=['q', 'v', 'k', 'o']\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "if parallel:\n",
    "    model = DataParallel(model) # Parallelize the model\n",
    "    print(\"Parallelized model\")\n",
    "model.to(device)  # Language modeling head to GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",  # Output directory\n",
    "    overwrite_output_dir=True,  # Overwrite the content of the output directory\n",
    "    num_train_epochs=100,  # Number of training epochs\n",
    "    per_device_train_batch_size=34,  # Batch size per device during training\n",
    "    gradient_accumulation_steps=42,  # Accumulate gradients over multiple steps\n",
    "    learning_rate=5e-5,  # Learning rate\n",
    "    save_steps=2000,  # Save checkpoint every 2000 steps\n",
    "    save_total_limit=2,  # Limit the total amount of checkpoints\n",
    "    eval_strategy=\"steps\",  # Evaluate during training at each `logging_steps`\n",
    "    logging_steps=500,  # Log every 500 steps\n",
    "    eval_steps=2000,  # Evaluate every 2000 steps\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"loss\",  # Use loss to evaluate the best model\n",
    "    predict_with_generate=True,  # Use generation for evaluation\n",
    "    fp16=fp16v,  # Load mixed precision training for CUDA only\n",
    "    remove_unused_columns=False,  # Remove unused columns from the dataset\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "#model eval\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "print(f\"Tokenizer Final Size = {len(tokenizer)}\")\n",
    "print(f\"Model Final Size = {model.get_input_embeddings().weight.shape[0]}\")\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(\"Training complete and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "results = trainer.evaluate(eval_dataset)\n",
    "print(\"Evaluation Results:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
