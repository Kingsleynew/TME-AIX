{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5G Network Operations Insights with Fine Tuning of EleutherAI/gpt-j-6B\n",
    "## Project Overview\n",
    "This project aims to deliver a 5g network insight with fine tuning a network performant LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "%pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import psutil\n",
    "import threading\n",
    "import sys\n",
    "import time\n",
    "from datasets import Dataset\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Set TOKENIZERS_PARALLELISM to false to avoid warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Check if CUDA (NVIDIA GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA (NVIDIA GPU)\")\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# Check if MPS (Apple Silicon GPU) is available\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Extract the .xz file\n",
    "with lzma.open('data/5G_netops_data.csv.xz', 'rb') as f_in:\n",
    "    with open('data/5G_netops_data.csv', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "# Load the synthetic telecom data\n",
    "data_path = \"data/5G_netops_data.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Define preprocessing function with actual column names\n",
    "def preprocess_function(examples):\n",
    "    input_texts = [\n",
    "        \" \".join([\n",
    "            f\"Date: {date}\",\n",
    "            f\"Cell Availability: {cell_avail}\",\n",
    "            f\"MTTR: {mttr}\",\n",
    "            f\"Throughput: {throughput}\",\n",
    "            f\"Latency: {latency}\",\n",
    "            f\"Packet Loss Rate: {packet_loss}\",\n",
    "            f\"Call Drop Rate: {call_drop}\",\n",
    "            f\"Handover Success Rate: {handover_success}\",\n",
    "            f\"Alarm Count: {alarm_count}\",\n",
    "            f\"Critical Alarm Count: {critical_alarm}\",\n",
    "            f\"Parameter Changes: {param_changes}\",\n",
    "            f\"Successful Configuration Changes: {success_config}\",\n",
    "            f\"Data Usage: {data_usage}\",\n",
    "            f\"User Count: {user_count}\",\n",
    "            f\"Signal Strength: {signal_strength}\",\n",
    "            f\"Jitter: {jitter}\",\n",
    "            f\"Connection Setup Success Rate: {connection_success}\",\n",
    "            f\"Security Incidents: {security_incidents}\",\n",
    "            f\"Authentication Failures: {auth_failures}\"\n",
    "        ]) for date, cell_avail, mttr, throughput, latency, packet_loss, call_drop, handover_success, alarm_count, critical_alarm, param_changes, success_config, data_usage, user_count, signal_strength, jitter, connection_success, security_incidents, auth_failures in zip(\n",
    "            examples['Date'], \n",
    "            examples['Cell Availability (%)'], \n",
    "            examples['MTTR (hours)'], \n",
    "            examples['Throughput (Mbps)'], \n",
    "            examples['Latency (ms)'], \n",
    "            examples['Packet Loss Rate (%)'], \n",
    "            examples['Call Drop Rate (%)'], \n",
    "            examples['Handover Success Rate (%)'], \n",
    "            examples['Alarm Count'], \n",
    "            examples['Critical Alarm Count'], \n",
    "            examples['Parameter Changes'], \n",
    "            examples['Successful Configuration Changes (%)'], \n",
    "            examples['Data Usage (GB)'], \n",
    "            examples['User Count'], \n",
    "            examples['Signal Strength (dBm)'], \n",
    "            examples['Jitter (ms)'], \n",
    "            examples['Connection Setup Success Rate (%)'], \n",
    "            examples['Security Incidents'], \n",
    "            examples['Authentication Failures'])\n",
    "    ]\n",
    "    target_texts = [str(fault_rate) for fault_rate in examples['Fault Occurrence Rate']]\n",
    "    \n",
    "    return {'input_text': input_texts, 'target_text': target_texts}\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_dataset = hf_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Model Structure\n",
    "model_name = \"EleutherAI/gpt-j-6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "model = GPTJForCausalLM.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples['input_text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    targets = tokenizer(examples['target_text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    return {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'labels': targets['input_ids']\n",
    "    }\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "columns = ['input_ids', 'attention_mask', 'labels']\n",
    "tokenized_dataset.set_format(type='torch', columns=columns)\n",
    "\n",
    "# Check the tokenized dataset\n",
    "print(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT Part\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Move model to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10, # 10 epochs\n",
    "    per_device_train_batch_size=1,  # Reduce batch size\n",
    "    gradient_accumulation_steps=16,  # Accumulate gradients over 16 steps\n",
    "    save_steps=10_000, # 10_000 steps \n",
    "    save_total_limit=5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Function to monitor system usage including GPU metrics\n",
    "def print_system_usage(stop_event):\n",
    "    while not stop_event.is_set():\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Usage: {torch.cuda.memory_allocated()} bytes\")\n",
    "            print(f\"GPU Utilization: {torch.cuda.utilization()}%\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            gpu_alloc_mem = int(torch.mps.current_allocated_memory()/1000000)\n",
    "            gpu_driver_mem = int(torch.mps.driver_allocated_memory()/1000000)\n",
    "        # Include other monitoring tools or custom logging as needed\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory_usage = psutil.virtual_memory().percent\n",
    "        sys.stdout.write(f\"\\rCPU Usage: {cpu_usage}% | Memory Usage: {memory_usage}% | GPU-Allocated-Memory Usage: {gpu_alloc_mem}MB | GPU-Driver-Memory Usage: {gpu_driver_mem}MB\")\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(60)  # Wait to Remeasure System Usage\n",
    "    while stop_event.is_set():\n",
    "        sys.stdout.write(f\"\\r Mr NAR Following The White Rabbit! \\r\")\n",
    "\n",
    "# Create an event to stop the thread\n",
    "stop_event = threading.Event()\n",
    "\n",
    "# Start the system usage monitoring thread\n",
    "monitoring_thread = threading.Thread(target=print_system_usage, args=(stop_event,))\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    # Training loop with monitoring\n",
    "    for epoch in range(training_args.num_train_epochs):\n",
    "        print(f\"\\rEpoch {epoch+1} starting...\\r\")\n",
    "        monitoring_thread.start()\n",
    "        trainer.train()\n",
    "        print(f\"\\rEpoch {epoch+1} finished.\\r\")\n",
    "finally:\n",
    "    # Stop the monitoring thread\n",
    "    stop_event.set()\n",
    "    monitoring_thread.join()\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
