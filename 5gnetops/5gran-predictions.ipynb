{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5G Network Operations Insights with Fine Tuning of T5-Small (This is the smallest version of T5.)\n",
    "## Project Overview\n",
    "Author: Fatih E. NAR<br>\n",
    "This project aims to deliver a 5g network insight with fine tuning a network performant encoder-decoder TransformerNN<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: pandas in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 3)) (3.9.0)\n",
      "Requirement already satisfied: tensorflow in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 4)) (2.16.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 5)) (1.5.0)\n",
      "Requirement already satisfied: onnx in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 6)) (1.16.1)\n",
      "Requirement already satisfied: tf2onnx in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 7)) (1.16.1)\n",
      "Requirement already satisfied: requests in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 8)) (2.32.2)\n",
      "Requirement already satisfied: transformers in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 9)) (4.41.2)\n",
      "Requirement already satisfied: datasets in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 10)) (2.19.1)\n",
      "Requirement already satisfied: peft in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 11)) (0.11.2.dev0)\n",
      "Requirement already satisfied: flask in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 12)) (3.0.3)\n",
      "Requirement already satisfied: torch in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 13)) (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 14)) (0.17.2)\n",
      "Requirement already satisfied: sentencepiece in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 15)) (0.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from pandas->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from matplotlib->-r requirements.txt (line 3)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from matplotlib->-r requirements.txt (line 3)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from matplotlib->-r requirements.txt (line 3)) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from matplotlib->-r requirements.txt (line 3)) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from matplotlib->-r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from matplotlib->-r requirements.txt (line 3)) (6.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from tensorflow->-r requirements.txt (line 4)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from tensorflow->-r requirements.txt (line 4)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from tensorflow->-r requirements.txt (line 4)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from tensorflow->-r requirements.txt (line 4)) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from tensorflow->-r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from tensorflow->-r requirements.txt (line 4)) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from tensorflow->-r requirements.txt (line 4)) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from tensorflow->-r requirements.txt (line 4)) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from tensorflow->-r requirements.txt (line 4)) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from tensorflow->-r requirements.txt (line 4)) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 4)) (58.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow->-r requirements.txt (line 4)) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from tensorflow->-r requirements.txt (line 4)) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from tensorflow->-r requirements.txt (line 4)) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from tensorflow->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from tensorflow->-r requirements.txt (line 4)) (1.64.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from tensorflow->-r requirements.txt (line 4)) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from tensorflow->-r requirements.txt (line 4)) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from tensorflow->-r requirements.txt (line 4)) (0.37.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from requests->-r requirements.txt (line 8)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from requests->-r requirements.txt (line 8)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from requests->-r requirements.txt (line 8)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from requests->-r requirements.txt (line 8)) (2024.2.2)\n",
      "Requirement already satisfied: filelock in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from transformers->-r requirements.txt (line 9)) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from transformers->-r requirements.txt (line 9)) (0.23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from transformers->-r requirements.txt (line 9)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from transformers->-r requirements.txt (line 9)) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from transformers->-r requirements.txt (line 9)) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from transformers->-r requirements.txt (line 9)) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from transformers->-r requirements.txt (line 9)) (4.66.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from datasets->-r requirements.txt (line 10)) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from datasets->-r requirements.txt (line 10)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from datasets->-r requirements.txt (line 10)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from datasets->-r requirements.txt (line 10)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from datasets->-r requirements.txt (line 10)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets->-r requirements.txt (line 10)) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from datasets->-r requirements.txt (line 10)) (3.9.5)\n",
      "Requirement already satisfied: psutil in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from peft->-r requirements.txt (line 11)) (5.9.8)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from peft->-r requirements.txt (line 11)) (0.30.1)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from flask->-r requirements.txt (line 12)) (3.0.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from flask->-r requirements.txt (line 12)) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from flask->-r requirements.txt (line 12)) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from flask->-r requirements.txt (line 12)) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from flask->-r requirements.txt (line 12)) (1.8.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from flask->-r requirements.txt (line 12)) (7.1.0)\n",
      "Requirement already satisfied: sympy in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from torch->-r requirements.txt (line 13)) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from torch->-r requirements.txt (line 13)) (3.2.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 4)) (0.37.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (4.0.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=3.6.0->flask->-r requirements.txt (line 12)) (3.18.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from Jinja2>=3.1.2->flask->-r requirements.txt (line 12)) (2.1.5)\n",
      "Requirement already satisfied: rich in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from keras>=3.0.0->tensorflow->-r requirements.txt (line 4)) (13.7.1)\n",
      "Requirement already satisfied: namex in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from keras>=3.0.0->tensorflow->-r requirements.txt (line 4)) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from keras>=3.0.0->tensorflow->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow->-r requirements.txt (line 4)) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow->-r requirements.txt (line 4)) (0.7.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from sympy->torch->-r requirements.txt (line 13)) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.0.0->tensorflow->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.0.0->tensorflow->-r requirements.txt (line 4)) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/fenar/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow->-r requirements.txt (line 4)) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fenar/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/fenar/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/fenar/Library/Python/3.9/lib/python/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "2024-06-03 09:25:23.776506: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 27 columns):\n",
      " #   Column                                Non-Null Count    Dtype  \n",
      "---  ------                                --------------    -----  \n",
      " 0   Season                                1000000 non-null  object \n",
      " 1   Cell Availability (%)                 1000000 non-null  float64\n",
      " 2   MTTR (hours)                          1000000 non-null  float64\n",
      " 3   Throughput (Mbps)                     1000000 non-null  float64\n",
      " 4   Latency (ms)                          1000000 non-null  float64\n",
      " 5   Packet Loss Rate (%)                  1000000 non-null  float64\n",
      " 6   Call Drop Rate (%)                    1000000 non-null  float64\n",
      " 7   Handover Success Rate (%)             1000000 non-null  float64\n",
      " 8   Alarm Count                           1000000 non-null  int64  \n",
      " 9   Critical Alarm Count                  1000000 non-null  int64  \n",
      " 10  Parameter Changes                     1000000 non-null  int64  \n",
      " 11  Successful Configuration Changes (%)  1000000 non-null  float64\n",
      " 12  Data Usage (GB)                       1000000 non-null  float64\n",
      " 13  User Count                            1000000 non-null  int64  \n",
      " 14  Signal Strength (dBm)                 1000000 non-null  float64\n",
      " 15  Jitter (ms)                           1000000 non-null  float64\n",
      " 16  Connection Setup Success Rate (%)     1000000 non-null  float64\n",
      " 17  Security Incidents                    1000000 non-null  int64  \n",
      " 18  Authentication Failures               1000000 non-null  int64  \n",
      " 19  Temperature (°C)                      1000000 non-null  float64\n",
      " 20  Humidity (%)                          1000000 non-null  float64\n",
      " 21  Weather                               1000000 non-null  object \n",
      " 22  Issue Reported                        1000000 non-null  object \n",
      " 23  City                                  1000000 non-null  object \n",
      " 24  State                                 1000000 non-null  object \n",
      " 25  Zip                                   1000000 non-null  int64  \n",
      " 26  Fault Occurrence Rate                 1000000 non-null  int64  \n",
      "dtypes: float64(14), int64(8), object(5)\n",
      "memory usage: 206.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Cell Availability (%)</th>\n",
       "      <th>MTTR (hours)</th>\n",
       "      <th>Throughput (Mbps)</th>\n",
       "      <th>Latency (ms)</th>\n",
       "      <th>Packet Loss Rate (%)</th>\n",
       "      <th>Call Drop Rate (%)</th>\n",
       "      <th>Handover Success Rate (%)</th>\n",
       "      <th>Alarm Count</th>\n",
       "      <th>Critical Alarm Count</th>\n",
       "      <th>...</th>\n",
       "      <th>Security Incidents</th>\n",
       "      <th>Authentication Failures</th>\n",
       "      <th>Temperature (°C)</th>\n",
       "      <th>Humidity (%)</th>\n",
       "      <th>Weather</th>\n",
       "      <th>Issue Reported</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Fault Occurrence Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fall</td>\n",
       "      <td>95.7495</td>\n",
       "      <td>5.81</td>\n",
       "      <td>18.7360</td>\n",
       "      <td>92.3200</td>\n",
       "      <td>1.6300</td>\n",
       "      <td>1.64255</td>\n",
       "      <td>96.65</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>16.82</td>\n",
       "      <td>38.56</td>\n",
       "      <td>Clear</td>\n",
       "      <td>no</td>\n",
       "      <td>Port Jennifer</td>\n",
       "      <td>VA</td>\n",
       "      <td>43568</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Winter</td>\n",
       "      <td>99.3145</td>\n",
       "      <td>7.91</td>\n",
       "      <td>-19.6452</td>\n",
       "      <td>61.2490</td>\n",
       "      <td>1.2054</td>\n",
       "      <td>2.78315</td>\n",
       "      <td>95.64</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38.52</td>\n",
       "      <td>53.13</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>no</td>\n",
       "      <td>East John</td>\n",
       "      <td>WA</td>\n",
       "      <td>56449</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Winter</td>\n",
       "      <td>92.2955</td>\n",
       "      <td>1.89</td>\n",
       "      <td>138.7740</td>\n",
       "      <td>74.4800</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1.97045</td>\n",
       "      <td>91.90</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>19.21</td>\n",
       "      <td>55.48</td>\n",
       "      <td>Clear</td>\n",
       "      <td>yes</td>\n",
       "      <td>Andreview</td>\n",
       "      <td>PR</td>\n",
       "      <td>77788</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Summer</td>\n",
       "      <td>97.4210</td>\n",
       "      <td>1.58</td>\n",
       "      <td>-126.9951</td>\n",
       "      <td>46.8395</td>\n",
       "      <td>5.1783</td>\n",
       "      <td>0.18300</td>\n",
       "      <td>97.49</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>24.83</td>\n",
       "      <td>87.41</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>no</td>\n",
       "      <td>Stacybury</td>\n",
       "      <td>GU</td>\n",
       "      <td>21375</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Summer</td>\n",
       "      <td>97.2150</td>\n",
       "      <td>1.10</td>\n",
       "      <td>29.3080</td>\n",
       "      <td>38.1225</td>\n",
       "      <td>3.2718</td>\n",
       "      <td>1.04080</td>\n",
       "      <td>92.47</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>37.49</td>\n",
       "      <td>61.02</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>no</td>\n",
       "      <td>East Laurastad</td>\n",
       "      <td>AR</td>\n",
       "      <td>4893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Season  Cell Availability (%)  MTTR (hours)  Throughput (Mbps)  \\\n",
       "0    Fall                95.7495          5.81            18.7360   \n",
       "1  Winter                99.3145          7.91           -19.6452   \n",
       "2  Winter                92.2955          1.89           138.7740   \n",
       "3  Summer                97.4210          1.58          -126.9951   \n",
       "4  Summer                97.2150          1.10            29.3080   \n",
       "\n",
       "   Latency (ms)  Packet Loss Rate (%)  Call Drop Rate (%)  \\\n",
       "0       92.3200                1.6300             1.64255   \n",
       "1       61.2490                1.2054             2.78315   \n",
       "2       74.4800                0.0100             1.97045   \n",
       "3       46.8395                5.1783             0.18300   \n",
       "4       38.1225                3.2718             1.04080   \n",
       "\n",
       "   Handover Success Rate (%)  Alarm Count  Critical Alarm Count  ...  \\\n",
       "0                      96.65            5                     2  ...   \n",
       "1                      95.64            6                     1  ...   \n",
       "2                      91.90            4                     1  ...   \n",
       "3                      97.49            4                     0  ...   \n",
       "4                      92.47            3                     0  ...   \n",
       "\n",
       "   Security Incidents  Authentication Failures  Temperature (°C)  \\\n",
       "0                   1                        8             16.82   \n",
       "1                   4                        0             38.52   \n",
       "2                   4                        7             19.21   \n",
       "3                   2                        7             24.83   \n",
       "4                   1                        9             37.49   \n",
       "\n",
       "   Humidity (%)  Weather  Issue Reported            City  State    Zip  \\\n",
       "0         38.56    Clear              no   Port Jennifer     VA  43568   \n",
       "1         53.13   Clouds              no       East John     WA  56449   \n",
       "2         55.48    Clear             yes       Andreview     PR  77788   \n",
       "3         87.41   Clouds              no       Stacybury     GU  21375   \n",
       "4         61.02   Clouds              no  East Laurastad     AR   4893   \n",
       "\n",
       "   Fault Occurrence Rate  \n",
       "0                      2  \n",
       "1                      1  \n",
       "2                      2  \n",
       "3                      2  \n",
       "4                      1  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lzma\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import threading\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "from datasets import Dataset\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments, get_linear_schedule_with_warmup, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model_save_path = \"models/5g_oss_model\"\n",
    "model_name = \"t5-small\"\n",
    "\n",
    "# Set TOKENIZERS_PARALLELISM to false to avoid warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Clear GPU cache before starting\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Cap memory usage to a specific size (e.g., 8 GB) for cuda\n",
    "max_memory_gb = 8\n",
    "max_memory_mb = max_memory_gb * 1024\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = f'max_split_size_mb:{max_memory_mb}'\n",
    "\n",
    "# Check if any accelerator is available \n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA (NVIDIA GPU)\")\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "    # Leverage multi-gpu if available\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "    # Clear GPU cache before starting\n",
    "    torch.cuda.empty_cache()\n",
    "# Check if MPS (Apple Silicon GPU) is available\n",
    "elif torch.backends.mps.is_available():\n",
    "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "    # Leverage multi-gpu if available\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Extract the .xz file\n",
    "with lzma.open('data/5G_netops_data_1M.csv.xz', 'rb') as f_in:\n",
    "    with open('data/5G_netops_data_1M.csv', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "# Load the synthetic telecom data\n",
    "data_path = \"data/5G_netops_data_1M.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information about the full dataset\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset size: 800000\n",
      "Evaluation Dataset size: 200000\n",
      "{'Season': 'Fall', 'Cell Availability (%)': 90.865, 'MTTR (hours)': 7.5, 'Throughput (Mbps)': -9.001500000000002, 'Latency (ms)': 68.77, 'Packet Loss Rate (%)': 3.6408, 'Call Drop Rate (%)': 2.0601, 'Handover Success Rate (%)': 96.74, 'Alarm Count': 11, 'Critical Alarm Count': 2, 'Parameter Changes': 12, 'Successful Configuration Changes (%)': 99.68, 'Data Usage (GB)': 31.68, 'User Count': 4873, 'Signal Strength (dBm)': -66.96, 'Jitter (ms)': 26.9, 'Connection Setup Success Rate (%)': 99.96, 'Security Incidents': 4, 'Authentication Failures': 8, 'Temperature (°C)': 34.51, 'Humidity (%)': 86.18, 'Weather': 'Clouds', 'Issue Reported': 'yes', 'City': 'Smithville', 'State': 'OK', 'Zip': '54923', 'Fault Occurrence Rate': 3, 'input_text': 'Season: Fall Cell Availability: 90.865 MTTR: 7.5 Throughput: -9.001500000000002 Latency: 68.77 Packet Loss Rate: 3.6408 Call Drop Rate: 2.0601 Handover Success Rate: 96.74 Alarm Count: 11 Critical Alarm Count: 2 Parameter Changes: 12 Successful Configuration Changes: 99.68 Data Usage: 31.68 User Count: 4873 Signal Strength: -66.96 Jitter: 26.9 Connection Setup Success Rate: 99.96 Security Incidents: 4 Authentication Failures: 8 Temperature: 34.51 Humidity: 86.18 Weather: Clouds Issue Reported: yes City: Smithville State: OK Zip: 54923', 'target_text': '3'}\n"
     ]
    }
   ],
   "source": [
    "# Fill NaN values and prepare input and target texts\n",
    "# Ensure all NaN values are filled with empty strings\n",
    "data = data.fillna('')\n",
    "\n",
    "# Ensure 'Zip' column is treated as a string\n",
    "data['Zip'] = data['Zip'].astype(str)\n",
    "\n",
    "# Prepare the input_text and target_text columns\n",
    "data['input_text'] = data.apply(lambda row: f\"Season: {row['Season']} Cell Availability: {row['Cell Availability (%)']} MTTR: {row['MTTR (hours)']} Throughput: {row['Throughput (Mbps)']} Latency: {row['Latency (ms)']} Packet Loss Rate: {row['Packet Loss Rate (%)']} Call Drop Rate: {row['Call Drop Rate (%)']} Handover Success Rate: {row['Handover Success Rate (%)']} Alarm Count: {row['Alarm Count']} Critical Alarm Count: {row['Critical Alarm Count']} Parameter Changes: {row['Parameter Changes']} Successful Configuration Changes: {row['Successful Configuration Changes (%)']} Data Usage: {row['Data Usage (GB)']} User Count: {row['User Count']} Signal Strength: {row['Signal Strength (dBm)']} Jitter: {row['Jitter (ms)']} Connection Setup Success Rate: {row['Connection Setup Success Rate (%)']} Security Incidents: {row['Security Incidents']} Authentication Failures: {row['Authentication Failures']} Temperature: {row['Temperature (°C)']} Humidity: {row['Humidity (%)']} Weather: {row['Weather']} Issue Reported: {row['Issue Reported']} City: {row['City']} State: {row['State']} Zip: {row['Zip']}\", axis=1)\n",
    "data['target_text'] = data['Fault Occurrence Rate'].astype(str)\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Split the dataset into training and evaluation\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "# Check the loaded dataset\n",
    "print(f\"Training Dataset size: {len(train_dataset)}\")\n",
    "print(f\"Evaluation Dataset size: {len(eval_dataset)}\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/800000 [00:00<?, ? examples/s]/Users/fenar/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 800000/800000 [10:55<00:00, 1220.84 examples/s]\n",
      "Map: 100%|██████████| 200000/200000 [02:48<00:00, 1188.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer from the pretrained model\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "# Match Tokenizer to the model\n",
    "tokenizer.add_tokens([f'<SPL_{i}' for i in range(0,28)])\n",
    "# Add the pad token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "else:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "#print len of tokenizer\n",
    "print(len(tokenizer))\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Tokenize datasets\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['input_text']\n",
    "    targets = examples['target_text']\n",
    "    model_inputs = tokenizer(inputs, max_length=512, padding='max_length', truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, padding='max_length', truncation=True)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "columns = ['input_ids', 'attention_mask', 'labels']\n",
    "train_dataset.set_format(type='torch', columns=columns)\n",
    "eval_dataset.set_format(type='torch', columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fenar/Library/Python/3.9/lib/python/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSeq2SeqLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): T5ForConditionalGeneration(\n",
       "      (shared): Embedding(32128, 512)\n",
       "      (encoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 512)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (relative_attention_bias): Embedding(32, 8)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-5): 5 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (decoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 512)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (relative_attention_bias): Embedding(32, 8)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-5): 5 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define PEFT/LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=4, # it was 2\n",
    "    lora_alpha=32, # it was 16\n",
    "    lora_dropout=0.1, # it was 0.05\n",
    "    target_modules=['q', 'v', 'k', 'o']\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.to(device)  # Language modeling head to GPU \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 500/13605 [54:44<23:52:57,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3141, 'grad_norm': 0.018248245120048523, 'learning_rate': 4.816244027930908e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1000/13605 [1:49:30<22:57:14,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0152, 'grad_norm': 0.006048723589628935, 'learning_rate': 4.6324880558618154e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1500/13605 [2:44:08<22:02:06,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0147, 'grad_norm': 0.005775829777121544, 'learning_rate': 4.448732083792723e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 2000/13605 [3:38:47<21:07:24,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0145, 'grad_norm': 0.004081073682755232, 'learning_rate': 4.264976111723631e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 15%|█▍        | 2000/13605 [5:19:02<21:07:24,  6.55s/it]/Users/fenar/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.014094140380620956, 'eval_runtime': 6014.8956, 'eval_samples_per_second': 33.251, 'eval_steps_per_second': 4.156, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2500/13605 [6:14:01<20:11:35,  6.55s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0144, 'grad_norm': 0.0053463950753211975, 'learning_rate': 4.081220139654539e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 3000/13605 [7:09:49<19:49:08,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0144, 'grad_norm': 0.005180933978408575, 'learning_rate': 3.897464167585447e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 3500/13605 [8:06:23<18:57:57,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0143, 'grad_norm': 0.0043270946480333805, 'learning_rate': 3.713708195516354e-05, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 4000/13605 [9:02:37<17:57:42,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0143, 'grad_norm': 0.006634572520852089, 'learning_rate': 3.529952223447262e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 29%|██▉       | 4000/13605 [9:50:02<17:57:42,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.014063416980206966, 'eval_runtime': 2845.3757, 'eval_samples_per_second': 70.289, 'eval_steps_per_second': 8.786, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fenar/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      " 33%|███▎      | 4500/13605 [10:46:14<17:01:21,  6.73s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0142, 'grad_norm': 0.0033459693659096956, 'learning_rate': 3.34619625137817e-05, 'epoch': 1.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 5000/13605 [11:42:24<16:04:40,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0142, 'grad_norm': 0.004528353456407785, 'learning_rate': 3.162440279309078e-05, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 5500/13605 [12:38:39<14:44:16,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0142, 'grad_norm': 0.006115181837230921, 'learning_rate': 2.9786843072399855e-05, 'epoch': 2.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 6000/13605 [13:33:15<13:50:16,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0141, 'grad_norm': 0.006036865524947643, 'learning_rate': 2.7949283351708934e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \n",
      " 44%|████▍     | 6000/13605 [14:20:44<13:50:16,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0139160742983222, 'eval_runtime': 2849.012, 'eval_samples_per_second': 70.2, 'eval_steps_per_second': 8.775, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fenar/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      " 48%|████▊     | 6500/13605 [15:15:21<12:55:54,  6.55s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0141, 'grad_norm': 0.006144976243376732, 'learning_rate': 2.611172363101801e-05, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 7000/13605 [16:09:57<12:01:24,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0141, 'grad_norm': 0.00606174860149622, 'learning_rate': 2.4274163910327085e-05, 'epoch': 2.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 7500/13605 [17:04:34<11:06:35,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0141, 'grad_norm': 0.006004191003739834, 'learning_rate': 2.2436604189636164e-05, 'epoch': 2.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 8000/13605 [17:59:10<10:11:59,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.014, 'grad_norm': 0.0038633786607533693, 'learning_rate': 2.059904446894524e-05, 'epoch': 2.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \n",
      " 59%|█████▉    | 8000/13605 [18:46:39<10:11:59,  6.55s/it]/Users/fenar/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.013824737630784512, 'eval_runtime': 2848.7985, 'eval_samples_per_second': 70.205, 'eval_steps_per_second': 8.776, 'epoch': 2.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 8500/13605 [19:41:20<9:23:15,  6.62s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.014, 'grad_norm': 0.00508967787027359, 'learning_rate': 1.876148474825432e-05, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 9000/13605 [20:36:00<8:23:24,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.014, 'grad_norm': 0.003918828908354044, 'learning_rate': 1.6923925027563398e-05, 'epoch': 3.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 9500/13605 [21:30:39<7:27:43,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0386, 'grad_norm': nan, 'learning_rate': 1.5086365306872474e-05, 'epoch': 3.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 10000/13605 [22:25:12<6:33:19,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 1.3248805586181551e-05, 'epoch': 3.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \n",
      " 74%|███████▎  | 10000/13605 [23:14:03<6:33:19,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 2931.6979, 'eval_samples_per_second': 68.22, 'eval_steps_per_second': 8.527, 'epoch': 3.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fenar/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      " 77%|███████▋  | 10500/13605 [24:08:42<5:38:53,  6.55s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 1.1411245865490629e-05, 'epoch': 3.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 11000/13605 [25:03:41<4:53:05,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 9.573686144799706e-06, 'epoch': 4.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 11500/13605 [25:59:59<3:59:10,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 7.736126424108783e-06, 'epoch': 4.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 12000/13605 [26:56:47<3:02:12,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 5.8985667034178614e-06, 'epoch': 4.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \n",
      " 88%|████████▊ | 12000/13605 [27:54:35<3:02:12,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 3468.047, 'eval_samples_per_second': 57.669, 'eval_steps_per_second': 7.209, 'epoch': 4.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fenar/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      " 92%|█████████▏| 12500/13605 [28:51:24<2:06:45,  6.88s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 4.061006982726939e-06, 'epoch': 4.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 13000/13605 [29:48:11<1:08:41,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.223447262036016e-06, 'epoch': 4.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 13500/13605 [30:44:59<11:56,  6.82s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 3.858875413450937e-07, 'epoch': 4.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13605/13605 [30:56:55<00:00,  8.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 111415.7272, 'train_samples_per_second': 35.902, 'train_steps_per_second': 0.122, 'train_loss': 0.02188537514240415, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13605, training_loss=0.02188537514240415, metrics={'train_runtime': 111415.7272, 'train_samples_per_second': 35.902, 'train_steps_per_second': 0.122, 'total_flos': 5.449646524763996e+17, 'train_loss': 0.02188537514240415, 'epoch': 4.999737505249895})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",  # Output directory\n",
    "    overwrite_output_dir=True,  # Overwrite the content of the output directory\n",
    "    num_train_epochs=5,  # Number of training epochs\n",
    "    per_device_train_batch_size=42,  # Batch size per device during training\n",
    "    gradient_accumulation_steps=7,  # Accumulate gradients over multiple steps\n",
    "    learning_rate=5e-5,  # Learning rate\n",
    "    save_steps=2000,  # Save checkpoint every 2000 steps\n",
    "    save_total_limit=2,  # Limit the total amount of checkpoints\n",
    "    eval_strategy=\"steps\",  # Evaluate during training at each `logging_steps`\n",
    "    logging_steps=500,  # Log every 500 steps\n",
    "    eval_steps=2000,  # Evaluate every 2000 steps\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"loss\",  # Use loss to evaluate the best model\n",
    "    predict_with_generate=True,  # Use generation for evaluation\n",
    "    fp16=False,  # Disable mixed precision training for MPS\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Final Size = 32128\n",
      "Model Final Size = 32128\n",
      "Training complete and model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fenar/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "print(f\"Tokenizer Final Size = {len(tokenizer)}\")\n",
    "print(f\"Model Final Size = {model.get_input_embeddings().weight.shape[0]}\")\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(\"Training complete and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [58:03<00:00,  7.18it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.013824737630784512, 'eval_runtime': 3483.8647, 'eval_samples_per_second': 57.408, 'eval_steps_per_second': 7.176, 'epoch': 4.999737505249895}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "results = trainer.evaluate(eval_dataset)\n",
    "print(\"Evaluation Results:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
